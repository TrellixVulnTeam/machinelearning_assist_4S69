{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "from typing import Dict, Any, Callable, List, Tuple, Optional, Union\n",
    "from sklearn import metrics\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer, DistilBertModel, DistilBertTokenizer\n",
    "from transformers import (AdamW, OpenAIGPTDoubleHeadsModel, OpenAIGPTTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_class = OpenAIGPTTokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(\"openai-gpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert DF to CONVERSATION Dictonary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df_to_conv_ai_dict(df: pd.DataFrame,\n",
    "                               personality: List[str],\n",
    "                               response_columns: List[str],\n",
    "                               tokenizer: Callable[[str], List[str]],\n",
    "                               max_tokens: Optional[int] = None,\n",
    "                               n_candidates: int = 6\n",
    "                               ) -> Dict[str, List[Any]]:  \n",
    "    # Add one because the index of the dataframe is the 0th position.\n",
    "    tuple_map = {name: index + 1 for index, name in enumerate(df.columns.tolist())}\n",
    "    train = []\n",
    "    val = []\n",
    "    # Step through every row in the dictionary\n",
    "    for row in df.itertuples():\n",
    "        question_text = row[tuple_map[\"body_1\"]]\n",
    "        for response_column in response_columns:\n",
    "            candidates = sample_candidates(df, row[tuple_map[\"id\"]], \"id\", \"body\", n_candidates)\n",
    "            # questions = sample_candidates(df, row[tuple_map[\"id\"]], \"id\", \"body_1\", n_candidates)\n",
    "            if max_tokens is not None: \n",
    "                questions = tokenizer.convert_tokens_to_string(tokenizer.tokenize(question_text)[:max_tokens])\n",
    "                candidates = [tokenizer.convert_tokens_to_string(tokenizer.tokenize(candidate)[:max_tokens]) for candidate in candidates]\n",
    "                d = {\"personality\": personality,\n",
    "                     \"utterances\": [{\"history\": questions,\n",
    "                                     \"candidates\": candidates}]}\n",
    "                if getattr(row, \"split\") == \"train\":\n",
    "                    train.append(d)\n",
    "                elif getattr(row, \"split\") == \"val\":\n",
    "                    val.append(d)\n",
    "                    \n",
    "    data = {\"train\": train, \"valid\": val}\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidate Sampling, see coment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_candidates(df: pd.DataFrame, current_id: Any, id_column: Any, text_column: str, n: int) -> List[str]:\n",
    "    \"\"\"Samples candidate responses to a question from the dataframe\n",
    "\n",
    "    It is aware of data splits and only samples from within the same split.  This avoids\n",
    "    leaking information between training validation and testing.  The sampled responses are\n",
    "    also drawn from all rows which do not have the same id as the current_id\n",
    "\n",
    "    Args:\n",
    "        df: The dataframe we want to sample responses from\n",
    "        current_id: The unique identifier we would like to leave out of our sampling\n",
    "        id_column: The column name in the dataframe with the unique ids.  current_id should\n",
    "            be an element of this column\n",
    "        text_column: The column with the text we want to sample\n",
    "        n: How many samples we want to take.\n",
    "\n",
    "    Returns:\n",
    "        A list of samples strings from our dataframe.\n",
    "    \"\"\"\n",
    "    # We must only sample candidates from the correct data split to avoid information leakage across channels\n",
    "    split = df[df[id_column] == current_id][\"split\"].tolist()[0]\n",
    "    candidate_df = df[df[\"split\"] == split]\n",
    "\n",
    "    # Sample 3 random rows from the dataframe not matching the current id\n",
    "    sampled_texts = candidate_df[candidate_df[id_column] != current_id].sample(n + 15)[text_column].tolist()\n",
    "\n",
    "    # join them all\n",
    "    text = \" \".join(sampled_texts)\n",
    "\n",
    "    # Replace all newlines with spaces...\n",
    "    text_no_newline = re.sub(\"\\n\", \" \", text).lower()\n",
    "\n",
    "    # Split on punctuation\n",
    "    split_text = re.split('[?.!]', text_no_newline)\n",
    "\n",
    "    # Remove all empty lines\n",
    "    filtered_text = [x.strip() for x in split_text if len(x.strip()) > 1]\n",
    "\n",
    "    # Shuffle the list\n",
    "    return np.random.choice(filtered_text, n).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataset from Bigquery which includes a siterip from Stackoverflow. The column body inherits all answers of the questions which are shown in the column body_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"answers.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'id', 'body', 'owner_user_id', 'body_1'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"split\"] = \"\"\n",
    "df_1 = df.split.apply(lambda x: random.choice(['train', 'val']) ) \n",
    "df['split'] = df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = convert_df_to_conv_ai_dict(df, [\"\"], [\"body\"], tokenizer, max_tokens=250, n_candidates=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tokens_stack.json\", \"w\") as json_file:\n",
    "    json.dump(d, json_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
